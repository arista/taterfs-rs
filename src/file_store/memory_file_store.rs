//! In-memory FileStore implementation for testing.

use super::chunk_sizes::next_chunk_size;
use super::scan_ignore_helper::ScanIgnoreHelper;
use crate::file_store::{
    DirEntry, DirectoryEntry, DirectoryScanEvent, Error, FileEntry, FileSource, FileStore, Result,
    ScanEvent, ScanEvents, SourceChunk, SourceChunkContent, SourceChunkContents, SourceChunks,
};
use crate::util::ManagedBuffers;
use async_trait::async_trait;
use bytes::Bytes;
use futures::{StreamExt, stream};
use sha2::{Digest, Sha256};
use std::collections::BTreeMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;

// =============================================================================
// MemoryFsEntry - Builder Input Types
// =============================================================================

/// An entry to be added to a MemoryFileStore via the builder.
pub enum MemoryFsEntry {
    /// A file with explicit contents.
    File { contents: Vec<u8>, executable: bool },
    /// A file with contents generated by repeating a pattern.
    Repeated {
        pattern: Vec<u8>,
        size: u64,
        executable: bool,
    },
    /// An empty directory.
    Dir,
}

impl MemoryFsEntry {
    /// Create a file with the given contents.
    pub fn file(contents: impl Into<Vec<u8>>) -> Self {
        MemoryFsEntry::File {
            contents: contents.into(),
            executable: false,
        }
    }

    /// Create an executable file with the given contents.
    pub fn executable(contents: impl Into<Vec<u8>>) -> Self {
        MemoryFsEntry::File {
            contents: contents.into(),
            executable: true,
        }
    }

    /// Create a file filled with a repeated pattern to the given size.
    pub fn repeated(pattern: &[u8], size: u64) -> Self {
        MemoryFsEntry::Repeated {
            pattern: pattern.to_vec(),
            size,
            executable: false,
        }
    }

    /// Create an executable file filled with a repeated pattern.
    pub fn repeated_executable(pattern: &[u8], size: u64) -> Self {
        MemoryFsEntry::Repeated {
            pattern: pattern.to_vec(),
            size,
            executable: true,
        }
    }

    /// Create an empty directory.
    pub fn dir() -> Self {
        MemoryFsEntry::Dir
    }
}

// =============================================================================
// Internal Tree Structure
// =============================================================================

/// Internal representation of a file in the memory store.
#[derive(Clone)]
struct MemoryFile {
    contents: MemoryFileContents,
    executable: bool,
}

/// How file contents are stored.
#[derive(Clone)]
enum MemoryFileContents {
    /// Explicit byte contents.
    Explicit(Arc<Vec<u8>>),
    /// Repeated pattern with total size.
    Repeated { pattern: Arc<Vec<u8>>, size: u64 },
}

impl MemoryFile {
    fn size(&self) -> u64 {
        match &self.contents {
            MemoryFileContents::Explicit(data) => data.len() as u64,
            MemoryFileContents::Repeated { size, .. } => *size,
        }
    }

    /// Read a range of bytes from the file.
    fn read_range(&self, offset: u64, length: u64) -> Vec<u8> {
        match &self.contents {
            MemoryFileContents::Explicit(data) => {
                let start = offset as usize;
                let end = (offset + length).min(data.len() as u64) as usize;
                data[start..end].to_vec()
            }
            MemoryFileContents::Repeated { pattern, size } => {
                let mut result = Vec::with_capacity(length as usize);
                let pattern_len = pattern.len() as u64;
                let mut pos = offset;
                let end = (offset + length).min(*size);

                while pos < end {
                    let pattern_offset = (pos % pattern_len) as usize;
                    let remaining_in_pattern = pattern_len - pattern_offset as u64;
                    let remaining_to_read = end - pos;
                    let to_copy = remaining_in_pattern.min(remaining_to_read) as usize;

                    result.extend_from_slice(&pattern[pattern_offset..pattern_offset + to_copy]);
                    pos += to_copy as u64;
                }
                result
            }
        }
    }

    /// Read all bytes from the file.
    fn read_all(&self) -> Vec<u8> {
        self.read_range(0, self.size())
    }
}

/// Internal node in the tree structure.
#[derive(Clone)]
enum TreeNode {
    File(MemoryFile),
    Directory(BTreeMap<String, TreeNode>),
}

// =============================================================================
// MemoryFileStore
// =============================================================================

/// An in-memory FileStore implementation, primarily for testing.
pub struct MemoryFileStore {
    root: BTreeMap<String, TreeNode>,
    /// Buffer manager for chunk allocation.
    managed_buffers: ManagedBuffers,
}

impl MemoryFileStore {
    /// Create a new builder for constructing a MemoryFileStore.
    pub fn builder() -> MemoryFileStoreBuilder {
        MemoryFileStoreBuilder::new()
    }

    /// Look up a node at the given path.
    fn get_node(&self, path: &Path) -> Option<&TreeNode> {
        let components: Vec<_> = path
            .components()
            .filter_map(|c| match c {
                std::path::Component::Normal(s) => s.to_str(),
                _ => None,
            })
            .collect();

        if components.is_empty() {
            return None; // Root is handled specially
        }

        let mut current = self.root.get(components[0])?;
        for component in &components[1..] {
            match current {
                TreeNode::Directory(children) => {
                    current = children.get(*component)?;
                }
                TreeNode::File(_) => return None,
            }
        }
        Some(current)
    }

    /// Check if a path is the root (empty path).
    fn is_root_path(path: &Path) -> bool {
        path.components()
            .filter(|c| matches!(c, std::path::Component::Normal(_)))
            .count()
            == 0
    }

    /// Convert a path to a string for entry paths.
    fn path_to_string(path: &Path) -> String {
        path.to_string_lossy().into_owned()
    }
}

// =============================================================================
// MemoryFileStoreBuilder
// =============================================================================

/// Builder for constructing a MemoryFileStore.
pub struct MemoryFileStoreBuilder {
    root: BTreeMap<String, TreeNode>,
    managed_buffers: ManagedBuffers,
}

impl MemoryFileStoreBuilder {
    fn new() -> Self {
        Self {
            root: BTreeMap::new(),
            managed_buffers: ManagedBuffers::new(),
        }
    }

    /// Set the ManagedBuffers for buffer allocation.
    pub fn with_managed_buffers(mut self, managed_buffers: ManagedBuffers) -> Self {
        self.managed_buffers = managed_buffers;
        self
    }

    /// Add an entry at the given path.
    ///
    /// Path components are separated by "/". Parent directories are created
    /// implicitly as needed.
    pub fn add(mut self, path: &str, entry: MemoryFsEntry) -> Self {
        let components: Vec<&str> = path.split('/').filter(|s| !s.is_empty()).collect();

        if components.is_empty() {
            return self;
        }

        // Navigate/create path to parent
        let mut current = &mut self.root;
        for component in &components[..components.len() - 1] {
            current = match current
                .entry((*component).to_string())
                .or_insert_with(|| TreeNode::Directory(BTreeMap::new()))
            {
                TreeNode::Directory(children) => children,
                TreeNode::File(_) => panic!("Cannot create directory inside file: {}", path),
            };
        }

        // Insert the final entry
        let name = components[components.len() - 1].to_string();
        let node = match entry {
            MemoryFsEntry::File {
                contents,
                executable,
            } => TreeNode::File(MemoryFile {
                contents: MemoryFileContents::Explicit(Arc::new(contents)),
                executable,
            }),
            MemoryFsEntry::Repeated {
                pattern,
                size,
                executable,
            } => TreeNode::File(MemoryFile {
                contents: MemoryFileContents::Repeated {
                    pattern: Arc::new(pattern),
                    size,
                },
                executable,
            }),
            MemoryFsEntry::Dir => TreeNode::Directory(BTreeMap::new()),
        };
        current.insert(name, node);

        self
    }

    /// Build the MemoryFileStore.
    pub fn build(self) -> MemoryFileStore {
        MemoryFileStore {
            root: self.root,
            managed_buffers: self.managed_buffers,
        }
    }
}

// =============================================================================
// SourceChunk Implementation
// =============================================================================

/// A chunk from a memory file.
struct MemorySourceChunk {
    file: MemoryFile,
    offset: u64,
    size: u64,
    managed_buffers: ManagedBuffers,
}

#[async_trait]
impl SourceChunk for MemorySourceChunk {
    fn offset(&self) -> u64 {
        self.offset
    }

    fn size(&self) -> u64 {
        self.size
    }

    async fn get(&self) -> Result<SourceChunkContent> {
        let data = self.file.read_range(self.offset, self.size);
        let hash = {
            let mut hasher = Sha256::new();
            hasher.update(&data);
            format!("{:x}", hasher.finalize())
        };
        let managed_buffer = self.managed_buffers.get_buffer_with_data(data).await;
        Ok(SourceChunkContent {
            offset: self.offset,
            size: self.size,
            bytes: Arc::new(managed_buffer),
            hash,
        })
    }
}

// =============================================================================
// FileSource Implementation
// =============================================================================

#[async_trait]
impl FileSource for MemoryFileStore {
    async fn scan(&self, path: Option<&Path>) -> Result<ScanEvents> {
        // Determine the starting point
        let (start_children, start_path) = match path {
            Some(p) if !Self::is_root_path(p) => {
                // Navigate to the specified path
                match self.get_node(p) {
                    Some(TreeNode::Directory(children)) => {
                        (children, p.to_path_buf())
                    }
                    Some(TreeNode::File(_)) => {
                        // Not a directory, return empty stream
                        return Ok(Box::pin(stream::empty()));
                    }
                    None => {
                        // Path not found, return empty stream
                        return Ok(Box::pin(stream::empty()));
                    }
                }
            }
            _ => (&self.root, PathBuf::new()),
        };

        let mut events = Vec::new();
        let mut helper = ScanIgnoreHelper::new();

        // Initialize helper by walking from root to the target path,
        // loading ignore files along the way
        helper.initialize_to_path(path, self).await;

        // Scan the tree with ignore filtering
        scan_tree_with_ignore(start_children, start_path, &mut events, &mut helper, self).await;

        Ok(Box::pin(stream::iter(events.into_iter().map(Ok))))
    }

    async fn get_source_chunks(&self, path: &Path) -> Result<Option<SourceChunks>> {
        let node = match self.get_node(path) {
            Some(n) => n,
            None => return Ok(None),
        };

        let file = match node {
            TreeNode::File(f) => f.clone(),
            TreeNode::Directory(_) => {
                return Err(Error::NotAFile(Self::path_to_string(path)));
            }
        };

        let chunks = compute_chunks(&file, self.managed_buffers.clone());
        Ok(Some(Box::pin(stream::iter(
            chunks
                .into_iter()
                .map(|c| Ok(Box::new(c) as Box<dyn SourceChunk>)),
        ))))
    }

    async fn get_source_chunk_contents(&self, chunks: SourceChunks) -> Result<SourceChunkContents> {
        // Sequential implementation - no concurrency for MemoryFileStore
        let contents_stream = chunks.then(|chunk_result| async move {
            let chunk = chunk_result?;
            chunk.get().await
        });
        Ok(Box::pin(contents_stream))
    }

    async fn get_entry(&self, path: &Path) -> Result<Option<DirectoryEntry>> {
        // Handle root path
        if Self::is_root_path(path) {
            return Ok(Some(DirectoryEntry::Dir(DirEntry {
                name: String::new(),
                path: String::new(),
            })));
        }

        let node = match self.get_node(path) {
            Some(n) => n,
            None => return Ok(None),
        };

        let name = path
            .file_name()
            .and_then(|s| s.to_str())
            .unwrap_or("")
            .to_string();
        let path_str = Self::path_to_string(path);

        Ok(Some(match node {
            TreeNode::File(f) => DirectoryEntry::File(FileEntry {
                name,
                path: path_str,
                size: f.size(),
                executable: f.executable,
                fingerprint: None,
            }),
            TreeNode::Directory(_) => DirectoryEntry::Dir(DirEntry {
                name,
                path: path_str,
            }),
        }))
    }

    async fn get_file(&self, path: &Path) -> Result<Bytes> {
        let node = self
            .get_node(path)
            .ok_or_else(|| Error::NotFound(Self::path_to_string(path)))?;

        match node {
            TreeNode::File(f) => Ok(Bytes::from(f.read_all())),
            TreeNode::Directory(_) => Err(Error::NotAFile(Self::path_to_string(path))),
        }
    }
}

impl FileStore for MemoryFileStore {
    fn get_source(&self) -> Option<&dyn FileSource> {
        Some(self)
    }

    fn get_dest(&self) -> Option<&dyn crate::file_store::FileDest> {
        None // Not implemented yet
    }
}

// =============================================================================
// Helper Functions
// =============================================================================

/// Recursively scan a tree with ignore filtering, generating scan events in depth-first, lexicographic order.
async fn scan_tree_with_ignore(
    children: &BTreeMap<String, TreeNode>,
    current_path: PathBuf,
    events: &mut Vec<ScanEvent>,
    helper: &mut ScanIgnoreHelper,
    source: &MemoryFileStore,
) {
    // BTreeMap iterates in sorted order
    for (name, node) in children {
        let is_dir = matches!(node, TreeNode::Directory(_));

        // Check if this entry should be ignored
        if helper.should_ignore(name, is_dir) {
            continue;
        }

        let entry_path = if current_path.as_os_str().is_empty() {
            PathBuf::from(name)
        } else {
            current_path.join(name)
        };
        let path_str = entry_path.to_string_lossy().into_owned();

        match node {
            TreeNode::Directory(dir_children) => {
                let dir_entry = DirEntry {
                    name: name.clone(),
                    path: path_str,
                };
                events.push(ScanEvent::EnterDirectory(dir_entry.clone()));

                // Notify helper of directory entry to load ignore files
                helper
                    .on_scan_event(&DirectoryScanEvent::EnterDirectory(dir_entry), source)
                    .await;

                Box::pin(scan_tree_with_ignore(
                    dir_children,
                    entry_path,
                    events,
                    helper,
                    source,
                ))
                .await;

                // Notify helper of directory exit
                helper
                    .on_scan_event(&DirectoryScanEvent::ExitDirectory, source)
                    .await;

                events.push(ScanEvent::ExitDirectory);
            }
            TreeNode::File(file) => {
                events.push(ScanEvent::File(FileEntry {
                    name: name.clone(),
                    path: path_str,
                    size: file.size(),
                    executable: file.executable,
                    fingerprint: None,
                }));
            }
        }
    }
}

/// Compute the chunk boundaries for a file.
fn compute_chunks(file: &MemoryFile, managed_buffers: ManagedBuffers) -> Vec<MemorySourceChunk> {
    let total_size = file.size();
    let mut chunks = Vec::new();
    let mut offset = 0u64;

    while offset < total_size {
        let remaining = total_size - offset;
        let chunk_size = next_chunk_size(remaining);

        chunks.push(MemorySourceChunk {
            file: file.clone(),
            offset,
            size: chunk_size,
            managed_buffers: managed_buffers.clone(),
        });

        offset += chunk_size;
    }

    chunks
}

// =============================================================================
// Tests
// =============================================================================

#[cfg(test)]
mod tests {
    use super::*;
    use futures::StreamExt;

    #[tokio::test]
    async fn test_empty_store() {
        let store = MemoryFileStore::builder().build();

        // Scan should yield no events
        let mut events = store.scan(None).await.unwrap();
        assert!(events.next().await.is_none());

        // Root should exist
        let root = store.get_entry(Path::new("")).await.unwrap();
        assert!(matches!(root, Some(DirectoryEntry::Dir(_))));

        // Non-existent path
        let missing = store.get_entry(Path::new("missing")).await.unwrap();
        assert!(missing.is_none());
    }

    #[tokio::test]
    async fn test_single_file() {
        let store = MemoryFileStore::builder()
            .add("hello.txt", MemoryFsEntry::file("Hello, World!"))
            .build();

        // Check entry
        let entry = store.get_entry(Path::new("hello.txt")).await.unwrap();
        match entry {
            Some(DirectoryEntry::File(f)) => {
                assert_eq!(f.name, "hello.txt");
                assert_eq!(f.size, 13);
                assert!(!f.executable);
            }
            _ => panic!("Expected file entry"),
        }

        // Read file contents
        let contents = store.get_file(Path::new("hello.txt")).await.unwrap();
        assert_eq!(&contents[..], b"Hello, World!");

        // Scan
        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;
        assert_eq!(events.len(), 1);
        assert!(matches!(&events[0], ScanEvent::File(f) if f.name == "hello.txt"));
    }

    #[tokio::test]
    async fn test_nested_directories() {
        let store = MemoryFileStore::builder()
            .add("a/b/c.txt", MemoryFsEntry::file("nested"))
            .add("a/d.txt", MemoryFsEntry::file("sibling"))
            .build();

        // Check nested file
        let entry = store.get_entry(Path::new("a/b/c.txt")).await.unwrap();
        assert!(matches!(entry, Some(DirectoryEntry::File(_))));

        // Check directory
        let dir = store.get_entry(Path::new("a/b")).await.unwrap();
        assert!(matches!(dir, Some(DirectoryEntry::Dir(_))));

        // Scan should be depth-first, lexicographic
        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        // Expected order: EnterDir(a), EnterDir(b), File(c.txt), ExitDir, File(d.txt), ExitDir
        assert_eq!(events.len(), 6);
        assert!(matches!(&events[0], ScanEvent::EnterDirectory(d) if d.name == "a"));
        assert!(matches!(&events[1], ScanEvent::EnterDirectory(d) if d.name == "b"));
        assert!(matches!(&events[2], ScanEvent::File(f) if f.name == "c.txt"));
        assert!(matches!(&events[3], ScanEvent::ExitDirectory));
        assert!(matches!(&events[4], ScanEvent::File(f) if f.name == "d.txt"));
        assert!(matches!(&events[5], ScanEvent::ExitDirectory));
    }

    #[tokio::test]
    async fn test_executable_file() {
        let store = MemoryFileStore::builder()
            .add("script.sh", MemoryFsEntry::executable("#!/bin/bash"))
            .build();

        let entry = store.get_entry(Path::new("script.sh")).await.unwrap();
        match entry {
            Some(DirectoryEntry::File(f)) => {
                assert!(f.executable);
            }
            _ => panic!("Expected file entry"),
        }
    }

    #[tokio::test]
    async fn test_repeated_file() {
        let store = MemoryFileStore::builder()
            .add("large.bin", MemoryFsEntry::repeated(b"ABCD", 1000))
            .build();

        let contents = store.get_file(Path::new("large.bin")).await.unwrap();
        assert_eq!(contents.len(), 1000);

        // Check pattern repeats correctly
        for (i, &byte) in contents.iter().enumerate() {
            let expected = b"ABCD"[i % 4];
            assert_eq!(byte, expected, "Mismatch at position {}", i);
        }
    }

    #[tokio::test]
    async fn test_chunking_small_file() {
        let store = MemoryFileStore::builder()
            .add("small.txt", MemoryFsEntry::file("tiny"))
            .build();

        let mut chunks = store
            .get_source_chunks(Path::new("small.txt"))
            .await
            .unwrap()
            .unwrap();

        let chunk = chunks.next().await.unwrap().unwrap();
        assert_eq!(chunk.offset(), 0);
        assert_eq!(chunk.size(), 4);

        let content = chunk.get().await.unwrap();
        assert_eq!(&content.bytes[..], b"tiny");

        // Should be only one chunk
        assert!(chunks.next().await.is_none());
    }

    #[tokio::test]
    async fn test_chunking_large_file() {
        // Create a file larger than 4MB to test chunking
        let size = 5_000_000u64; // ~5MB
        let store = MemoryFileStore::builder()
            .add("large.bin", MemoryFsEntry::repeated(b"X", size))
            .build();

        let chunks: Vec<_> = store
            .get_source_chunks(Path::new("large.bin"))
            .await
            .unwrap()
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        // First chunk should be 4MB
        assert_eq!(chunks[0].offset(), 0);
        assert_eq!(chunks[0].size(), 4_194_304);

        // Remaining ~806KB should be broken down further
        let total_size: u64 = chunks.iter().map(|c| c.size()).sum();
        assert_eq!(total_size, size);

        // Verify chunk content hashes are computed
        let content = chunks[0].get().await.unwrap();
        assert!(!content.hash.is_empty());
        assert_eq!(content.hash.len(), 64); // SHA-256 hex
    }

    #[tokio::test]
    async fn test_lexicographic_order() {
        let store = MemoryFileStore::builder()
            .add("z.txt", MemoryFsEntry::file("z"))
            .add("a.txt", MemoryFsEntry::file("a"))
            .add("m.txt", MemoryFsEntry::file("m"))
            .build();

        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert_eq!(names, vec!["a.txt", "m.txt", "z.txt"]);
    }

    #[tokio::test]
    async fn test_get_source_chunks_not_found() {
        let store = MemoryFileStore::builder().build();
        let result = store.get_source_chunks(Path::new("missing")).await.unwrap();
        assert!(result.is_none());
    }

    #[tokio::test]
    async fn test_get_source_chunks_on_directory() {
        let store = MemoryFileStore::builder()
            .add("dir/file.txt", MemoryFsEntry::file("content"))
            .build();

        let result = store.get_source_chunks(Path::new("dir")).await;
        assert!(matches!(result, Err(Error::NotAFile(_))));
    }

    #[tokio::test]
    async fn test_get_file_not_found() {
        let store = MemoryFileStore::builder().build();
        let result = store.get_file(Path::new("missing")).await;
        assert!(matches!(result, Err(Error::NotFound(_))));
    }

    #[tokio::test]
    async fn test_get_file_on_directory() {
        let store = MemoryFileStore::builder()
            .add("dir/file.txt", MemoryFsEntry::file("content"))
            .build();

        let result = store.get_file(Path::new("dir")).await;
        assert!(matches!(result, Err(Error::NotAFile(_))));
    }

    // =========================================================================
    // Ignore Functionality Tests
    // =========================================================================

    #[tokio::test]
    async fn test_scan_ignores_git_directory() {
        let store = MemoryFileStore::builder()
            .add(".git/config", MemoryFsEntry::file("git config"))
            .add(".git/HEAD", MemoryFsEntry::file("ref: refs/heads/main"))
            .add("src/main.rs", MemoryFsEntry::file("fn main() {}"))
            .build();

        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        // .git directory should not appear in scan
        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::EnterDirectory(d) => Some(d.name.as_str()),
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert!(!names.contains(&".git"));
        assert!(names.contains(&"src"));
        assert!(names.contains(&"main.rs"));
    }

    #[tokio::test]
    async fn test_scan_ignores_tfs_directory() {
        let store = MemoryFileStore::builder()
            .add(".tfs/data", MemoryFsEntry::file("tfs data"))
            .add("file.txt", MemoryFsEntry::file("content"))
            .build();

        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::EnterDirectory(d) => Some(d.name.as_str()),
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert!(!names.contains(&".tfs"));
        assert!(names.contains(&"file.txt"));
    }

    #[tokio::test]
    async fn test_scan_respects_gitignore() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log\ntarget/"))
            .add("app.log", MemoryFsEntry::file("log content"))
            .add("main.rs", MemoryFsEntry::file("fn main() {}"))
            .add("target/debug/app", MemoryFsEntry::file("binary"))
            .build();

        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::EnterDirectory(d) => Some(d.name.as_str()),
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        // .gitignore itself should be visible
        assert!(names.contains(&".gitignore"));
        // Ignored files should not appear
        assert!(!names.contains(&"app.log"));
        assert!(!names.contains(&"target"));
        // Non-ignored files should appear
        assert!(names.contains(&"main.rs"));
    }

    #[tokio::test]
    async fn test_scan_respects_tfsignore() {
        let store = MemoryFileStore::builder()
            .add(".tfsignore", MemoryFsEntry::file("*.tmp"))
            .add("data.tmp", MemoryFsEntry::file("temp"))
            .add("data.txt", MemoryFsEntry::file("permanent"))
            .build();

        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert!(!names.contains(&"data.tmp"));
        assert!(names.contains(&"data.txt"));
    }

    #[tokio::test]
    async fn test_scan_nested_gitignore() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log"))
            .add("src/.gitignore", MemoryFsEntry::file("*.bak"))
            .add("root.log", MemoryFsEntry::file("ignored"))
            .add("root.txt", MemoryFsEntry::file("visible"))
            .add("src/file.bak", MemoryFsEntry::file("ignored"))
            .add("src/file.rs", MemoryFsEntry::file("visible"))
            .add("src/app.log", MemoryFsEntry::file("also ignored"))
            .build();

        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        // Root level ignore
        assert!(!names.contains(&"root.log"));
        assert!(names.contains(&"root.txt"));

        // Nested ignore (both root and nested patterns apply)
        assert!(!names.contains(&"file.bak"));
        assert!(!names.contains(&"app.log"));
        assert!(names.contains(&"file.rs"));
    }

    #[tokio::test]
    async fn test_scan_negation_pattern() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log\n!important.log"))
            .add("debug.log", MemoryFsEntry::file("ignored"))
            .add("important.log", MemoryFsEntry::file("not ignored"))
            .build();

        let events: Vec<_> = store
            .scan(None)
            .await
            .unwrap()
            .map(|r| r.unwrap())
            .collect()
            .await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert!(!names.contains(&"debug.log"));
        assert!(names.contains(&"important.log"));
    }
}
