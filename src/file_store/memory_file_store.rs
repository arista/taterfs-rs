//! In-memory FileStore implementation for testing.

use super::chunk_sizes::next_chunk_size;
use super::scan_ignore_helper::{ScanDirEntry, ScanDirectoryEvent, ScanIgnoreHelper};
use crate::caches::{FileStoreCache, NoopFileStoreCache};
use crate::file_store::{
    DirEntry, DirectoryEntry, DirectoryList, DirectoryListSource, Error, FileEntry, FileSource,
    FileStore, Result, ScanEvent, ScanEvents, SourceChunk, SourceChunkContent,
    SourceChunkList, SourceChunkWithContent, SourceChunkWithContentList, SourceChunks,
    SourceChunksWithContent, VecScanEventList,
};
use crate::util::ManagedBuffers;
use async_trait::async_trait;
use bytes::Bytes;
use sha2::{Digest, Sha256};
use std::collections::BTreeMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokio::sync::RwLock;

// =============================================================================
// MemoryFsEntry - Builder Input Types
// =============================================================================

/// An entry to be added to a MemoryFileStore via the builder.
pub enum MemoryFsEntry {
    /// A file with explicit contents.
    File { contents: Vec<u8>, executable: bool },
    /// A file with contents generated by repeating a pattern.
    Repeated {
        pattern: Vec<u8>,
        size: u64,
        executable: bool,
    },
    /// An empty directory.
    Dir,
}

impl MemoryFsEntry {
    /// Create a file with the given contents.
    pub fn file(contents: impl Into<Vec<u8>>) -> Self {
        MemoryFsEntry::File {
            contents: contents.into(),
            executable: false,
        }
    }

    /// Create an executable file with the given contents.
    pub fn executable(contents: impl Into<Vec<u8>>) -> Self {
        MemoryFsEntry::File {
            contents: contents.into(),
            executable: true,
        }
    }

    /// Create a file filled with a repeated pattern to the given size.
    pub fn repeated(pattern: &[u8], size: u64) -> Self {
        MemoryFsEntry::Repeated {
            pattern: pattern.to_vec(),
            size,
            executable: false,
        }
    }

    /// Create an executable file filled with a repeated pattern.
    pub fn repeated_executable(pattern: &[u8], size: u64) -> Self {
        MemoryFsEntry::Repeated {
            pattern: pattern.to_vec(),
            size,
            executable: true,
        }
    }

    /// Create an empty directory.
    pub fn dir() -> Self {
        MemoryFsEntry::Dir
    }
}

// =============================================================================
// Internal Tree Structure
// =============================================================================

/// Internal representation of a file in the memory store.
#[derive(Clone)]
struct MemoryFile {
    contents: MemoryFileContents,
    executable: bool,
}

/// How file contents are stored.
#[derive(Clone)]
enum MemoryFileContents {
    /// Explicit byte contents.
    Explicit(Arc<Vec<u8>>),
    /// Repeated pattern with total size.
    Repeated { pattern: Arc<Vec<u8>>, size: u64 },
}

impl MemoryFile {
    fn size(&self) -> u64 {
        match &self.contents {
            MemoryFileContents::Explicit(data) => data.len() as u64,
            MemoryFileContents::Repeated { size, .. } => *size,
        }
    }

    /// Read a range of bytes from the file.
    fn read_range(&self, offset: u64, length: u64) -> Vec<u8> {
        match &self.contents {
            MemoryFileContents::Explicit(data) => {
                let start = offset as usize;
                let end = (offset + length).min(data.len() as u64) as usize;
                data[start..end].to_vec()
            }
            MemoryFileContents::Repeated { pattern, size } => {
                let mut result = Vec::with_capacity(length as usize);
                let pattern_len = pattern.len() as u64;
                let mut pos = offset;
                let end = (offset + length).min(*size);

                while pos < end {
                    let pattern_offset = (pos % pattern_len) as usize;
                    let remaining_in_pattern = pattern_len - pattern_offset as u64;
                    let remaining_to_read = end - pos;
                    let to_copy = remaining_in_pattern.min(remaining_to_read) as usize;

                    result.extend_from_slice(&pattern[pattern_offset..pattern_offset + to_copy]);
                    pos += to_copy as u64;
                }
                result
            }
        }
    }

    /// Read all bytes from the file.
    fn read_all(&self) -> Vec<u8> {
        self.read_range(0, self.size())
    }
}

/// Internal node in the tree structure.
#[derive(Clone)]
enum TreeNode {
    File(MemoryFile),
    Directory(BTreeMap<String, TreeNode>),
}

// =============================================================================
// MemoryFileStore
// =============================================================================

/// An in-memory FileStore implementation, primarily for testing.
pub struct MemoryFileStore {
    root: RwLock<BTreeMap<String, TreeNode>>,
    /// Buffer manager for chunk allocation.
    managed_buffers: ManagedBuffers,
    /// Cache for this file store.
    cache: Arc<dyn FileStoreCache>,
}

impl MemoryFileStore {
    /// Create a new builder for constructing a MemoryFileStore.
    pub fn builder() -> MemoryFileStoreBuilder {
        MemoryFileStoreBuilder::new()
    }

    /// Check if a path is the root (empty path).
    fn is_root_path(path: &Path) -> bool {
        path.components()
            .filter(|c| matches!(c, std::path::Component::Normal(_)))
            .count()
            == 0
    }

    /// Convert a path to a string for entry paths.
    fn path_to_string(path: &Path) -> String {
        path.to_string_lossy().into_owned()
    }
}

/// Look up a node at the given path in the tree.
fn get_node<'a>(root: &'a BTreeMap<String, TreeNode>, path: &Path) -> Option<&'a TreeNode> {
    let components: Vec<_> = path
        .components()
        .filter_map(|c| match c {
            std::path::Component::Normal(s) => s.to_str(),
            _ => None,
        })
        .collect();

    if components.is_empty() {
        return None; // Root is handled specially
    }

    let mut current = root.get(components[0])?;
    for component in &components[1..] {
        match current {
            TreeNode::Directory(children) => {
                current = children.get(*component)?;
            }
            TreeNode::File(_) => return None,
        }
    }
    Some(current)
}

/// Extract path components as strings.
fn path_components(path: &Path) -> Vec<&str> {
    path.components()
        .filter_map(|c| match c {
            std::path::Component::Normal(s) => s.to_str(),
            _ => None,
        })
        .collect()
}

/// Navigate to the parent directory of a path, creating intermediate
/// directories as needed. Returns a mutable reference to the parent's children map.
fn get_or_create_parent<'a>(
    root: &'a mut BTreeMap<String, TreeNode>,
    parent_components: &[&str],
) -> Result<&'a mut BTreeMap<String, TreeNode>> {
    let mut current = root;
    for &component in parent_components {
        current = match current
            .entry(component.to_string())
            .or_insert_with(|| TreeNode::Directory(BTreeMap::new()))
        {
            TreeNode::Directory(children) => children,
            TreeNode::File(_) => {
                return Err(Error::NotADirectory(component.to_string()));
            }
        };
    }
    Ok(current)
}

/// Navigate to a mutable node at the given path.
fn get_node_mut<'a>(
    root: &'a mut BTreeMap<String, TreeNode>,
    path: &Path,
) -> Option<&'a mut TreeNode> {
    let components = path_components(path);
    if components.is_empty() {
        return None;
    }

    let mut current = root.get_mut(components[0])?;
    for component in &components[1..] {
        match current {
            TreeNode::Directory(children) => {
                current = children.get_mut(*component)?;
            }
            TreeNode::File(_) => return None,
        }
    }
    Some(current)
}

// =============================================================================
// MemoryFileStoreBuilder
// =============================================================================

/// Builder for constructing a MemoryFileStore.
pub struct MemoryFileStoreBuilder {
    root: BTreeMap<String, TreeNode>,
    cache: Option<Arc<dyn FileStoreCache>>,
    managed_buffers: Option<ManagedBuffers>,
}

impl MemoryFileStoreBuilder {
    fn new() -> Self {
        Self {
            root: BTreeMap::new(),
            cache: None,
            managed_buffers: None,
        }
    }

    /// Set the cache for this file store.
    pub fn with_cache(mut self, cache: Arc<dyn FileStoreCache>) -> Self {
        self.cache = Some(cache);
        self
    }

    /// Set the managed buffers for this file store.
    pub fn with_managed_buffers(mut self, managed_buffers: ManagedBuffers) -> Self {
        self.managed_buffers = Some(managed_buffers);
        self
    }

    /// Add an entry at the given path.
    ///
    /// Path components are separated by "/". Parent directories are created
    /// implicitly as needed.
    pub fn add(mut self, path: &str, entry: MemoryFsEntry) -> Self {
        let components: Vec<&str> = path.split('/').filter(|s| !s.is_empty()).collect();

        if components.is_empty() {
            return self;
        }

        // Navigate/create path to parent
        let mut current = &mut self.root;
        for component in &components[..components.len() - 1] {
            current = match current
                .entry((*component).to_string())
                .or_insert_with(|| TreeNode::Directory(BTreeMap::new()))
            {
                TreeNode::Directory(children) => children,
                TreeNode::File(_) => panic!("Cannot create directory inside file: {}", path),
            };
        }

        // Insert the final entry
        let name = components[components.len() - 1].to_string();
        let node = match entry {
            MemoryFsEntry::File {
                contents,
                executable,
            } => TreeNode::File(MemoryFile {
                contents: MemoryFileContents::Explicit(Arc::new(contents)),
                executable,
            }),
            MemoryFsEntry::Repeated {
                pattern,
                size,
                executable,
            } => TreeNode::File(MemoryFile {
                contents: MemoryFileContents::Repeated {
                    pattern: Arc::new(pattern),
                    size,
                },
                executable,
            }),
            MemoryFsEntry::Dir => TreeNode::Directory(BTreeMap::new()),
        };
        current.insert(name, node);

        self
    }

    /// Build the MemoryFileStore.
    pub fn build(self) -> MemoryFileStore {
        MemoryFileStore {
            root: RwLock::new(self.root),
            managed_buffers: self.managed_buffers.unwrap_or_default(),
            cache: self.cache.unwrap_or_else(|| Arc::new(NoopFileStoreCache)),
        }
    }
}

// =============================================================================
// Chunk List Implementation
// =============================================================================

/// SourceChunkList implementation for MemoryFileStore.
///
/// Computes chunks iteratively as next() is called.
pub struct MemorySourceChunkList {
    file_size: u64,
    current_offset: u64,
}

impl MemorySourceChunkList {
    fn new(file_size: u64) -> Self {
        Self {
            file_size,
            current_offset: 0,
        }
    }
}

#[async_trait]
impl SourceChunkList for MemorySourceChunkList {
    async fn next(&mut self) -> Option<Result<SourceChunk>> {
        if self.current_offset >= self.file_size {
            return None;
        }
        let remaining = self.file_size - self.current_offset;
        let size = next_chunk_size(remaining);
        let chunk = SourceChunk {
            offset: self.current_offset,
            size,
        };
        self.current_offset += size;
        Some(Ok(chunk))
    }
}

// =============================================================================
// Chunk Content Helpers
// =============================================================================

/// Read a chunk from a memory file and create a SourceChunkContent.
///
/// Uses the acquire-then-read pattern: acquires buffer capacity BEFORE
/// reading the content, ensuring backpressure is applied before I/O.
async fn read_chunk_content(
    file: &MemoryFile,
    offset: u64,
    size: u64,
    managed_buffers: &ManagedBuffers,
) -> Result<SourceChunkContent> {
    // 1. Acquire capacity before reading (waits if capacity limit is reached)
    let acquired = managed_buffers.acquire(size).await;

    // 2. Read the data
    let data = file.read_range(offset, size);

    // 3. Compute hash
    let hash = {
        let mut hasher = Sha256::new();
        hasher.update(&data);
        format!("{:x}", hasher.finalize())
    };

    // 4. Create buffer using the acquired capacity (doesn't wait)
    let managed_buffer = managed_buffers.create_buffer_with_acquired(data, acquired);

    Ok(SourceChunkContent {
        offset,
        size,
        bytes: Arc::new(managed_buffer),
        hash,
    })
}

/// Sequential implementation of SourceChunkWithContentList for MemoryFileStore.
///
/// This doesn't do any concurrency - it reads chunks sequentially.
/// Chunks are computed iteratively as next() is called.
struct MemorySourceChunkWithContentList {
    file: MemoryFile,
    file_size: u64,
    current_offset: u64,
    managed_buffers: ManagedBuffers,
}

impl MemorySourceChunkWithContentList {
    fn new(file: MemoryFile, file_size: u64, managed_buffers: ManagedBuffers) -> Self {
        Self {
            file,
            file_size,
            current_offset: 0,
            managed_buffers,
        }
    }
}

#[async_trait]
impl SourceChunkWithContentList for MemorySourceChunkWithContentList {
    async fn next(&mut self) -> Option<Result<SourceChunkWithContent>> {
        if self.current_offset >= self.file_size {
            return None;
        }

        let remaining = self.file_size - self.current_offset;
        let size = next_chunk_size(remaining);
        let offset = self.current_offset;
        self.current_offset += size;

        // For MemoryFileStore, we read synchronously (no background download needed)
        let content =
            match read_chunk_content(&self.file, offset, size, &self.managed_buffers).await {
                Ok(c) => c,
                Err(e) => return Some(Err(e)),
            };

        Some(Ok(SourceChunkWithContent::new_immediate(
            offset, size, content,
        )))
    }
}

// =============================================================================
// FileSource Implementation
// =============================================================================

#[async_trait]
impl FileSource for MemoryFileStore {
    async fn scan(&self, path: Option<&Path>) -> Result<ScanEvents> {
        // Clone root to avoid holding the lock during the scan.
        // This prevents deadlocks since scan_tree_with_ignore needs
        // ScanFileSource access (which would re-acquire the lock).
        let source = MemoryDirectoryListSource {
            root: self.root.read().await.clone(),
        };

        // Determine the starting point
        let (start_children, start_path) = match path {
            Some(p) if !Self::is_root_path(p) => match get_node(&source.root, p) {
                Some(TreeNode::Directory(children)) => (children, p.to_path_buf()),
                Some(TreeNode::File(_)) => {
                    return Err(Error::NotADirectory(Self::path_to_string(p)));
                }
                None => {
                    return Err(Error::NotFound(Self::path_to_string(p)));
                }
            },
            _ => (&source.root, PathBuf::new()),
        };

        let mut events = Vec::new();
        let mut helper = ScanIgnoreHelper::new();

        // Initialize helper by walking from root to the target path,
        // loading ignore files along the way
        helper.initialize_to_path(path, &source).await;

        // Scan the tree with ignore filtering
        scan_tree_with_ignore(
            start_children,
            start_path,
            &mut events,
            &mut helper,
            &source,
        )
        .await;

        Ok(Box::new(VecScanEventList::new(events)))
    }

    async fn get_source_chunks(&self, path: &Path) -> Result<Option<SourceChunks>> {
        let root = self.root.read().await;
        let node = match get_node(&root, path) {
            Some(n) => n,
            None => return Ok(None),
        };

        let file_size = match node {
            TreeNode::File(f) => f.size(),
            TreeNode::Directory(_) => {
                return Err(Error::NotAFile(Self::path_to_string(path)));
            }
        };

        Ok(Some(Box::new(MemorySourceChunkList::new(file_size))))
    }

    async fn get_source_chunks_with_content(
        &self,
        path: &Path,
    ) -> Result<Option<SourceChunksWithContent>> {
        let root = self.root.read().await;
        let node = match get_node(&root, path) {
            Some(n) => n,
            None => return Ok(None),
        };

        let file = match node {
            TreeNode::File(f) => f.clone(),
            TreeNode::Directory(_) => {
                return Err(Error::NotAFile(Self::path_to_string(path)));
            }
        };

        let file_size = file.size();

        Ok(Some(Box::new(MemorySourceChunkWithContentList::new(
            file,
            file_size,
            self.managed_buffers.clone(),
        ))))
    }

    async fn get_entry(&self, path: &Path) -> Result<Option<DirectoryEntry>> {
        // Handle root path
        if Self::is_root_path(path) {
            return Ok(Some(DirectoryEntry::Dir(DirEntry {
                name: String::new(),
                path: String::new(),
            })));
        }

        let root = self.root.read().await;
        let node = match get_node(&root, path) {
            Some(n) => n,
            None => return Ok(None),
        };

        let name = path
            .file_name()
            .and_then(|s| s.to_str())
            .unwrap_or("")
            .to_string();
        let path_str = Self::path_to_string(path);

        Ok(Some(match node {
            TreeNode::File(f) => DirectoryEntry::File(FileEntry {
                name,
                path: path_str,
                size: f.size(),
                executable: f.executable,
                fingerprint: None,
            }),
            TreeNode::Directory(_) => DirectoryEntry::Dir(DirEntry {
                name,
                path: path_str,
            }),
        }))
    }

    async fn get_file(&self, path: &Path) -> Result<Bytes> {
        let root = self.root.read().await;
        let node =
            get_node(&root, path).ok_or_else(|| Error::NotFound(Self::path_to_string(path)))?;

        match node {
            TreeNode::File(f) => Ok(Bytes::from(f.read_all())),
            TreeNode::Directory(_) => Err(Error::NotAFile(Self::path_to_string(path))),
        }
    }
}

impl FileStore for MemoryFileStore {
    fn get_source(&self) -> Option<&dyn FileSource> {
        Some(self)
    }

    fn get_dest(&self) -> Option<&dyn crate::file_store::FileDest> {
        Some(self)
    }

    fn get_cache(&self) -> Arc<dyn FileStoreCache> {
        self.cache.clone()
    }
}

// =============================================================================
// DirectoryListSource Implementation
// =============================================================================

/// A DirectoryListSource backed by the MemoryFileStore's tree data.
struct MemoryDirectoryListSource {
    root: BTreeMap<String, TreeNode>,
}

impl MemoryDirectoryListSource {
    /// Navigate to a directory in the tree and return its children.
    fn get_children(&self, path: &str) -> Option<&BTreeMap<String, TreeNode>> {
        if path.is_empty() {
            return Some(&self.root);
        }
        let components: Vec<&str> = path.split('/').filter(|s| !s.is_empty()).collect();
        let mut current = &self.root;
        for component in &components {
            match current.get(*component)? {
                TreeNode::Directory(children) => current = children,
                TreeNode::File(_) => return None,
            }
        }
        Some(current)
    }
}

#[async_trait]
impl super::scan_ignore_helper::ScanFileSource for MemoryDirectoryListSource {
    async fn get_file(
        &self,
        path: &Path,
    ) -> std::result::Result<Bytes, Box<dyn std::error::Error + Send + Sync>> {
        let path_str = path.to_string_lossy();
        let components: Vec<&str> = path_str.split('/').filter(|s| !s.is_empty()).collect();
        if components.is_empty() {
            return Err("empty path".into());
        }
        let mut current = &self.root;
        for component in &components[..components.len() - 1] {
            match current.get(*component) {
                Some(TreeNode::Directory(children)) => current = children,
                _ => return Err(format!("not found: {}", path_str).into()),
            }
        }
        match current.get(components[components.len() - 1]) {
            Some(TreeNode::File(f)) => Ok(Bytes::from(f.read_all())),
            _ => Err(format!("not found: {}", path_str).into()),
        }
    }
}

#[async_trait]
impl DirectoryListSource for MemoryDirectoryListSource {
    async fn list_raw_directory(&self, path: &str) -> Result<Option<Vec<DirectoryEntry>>> {
        let children = match self.get_children(path) {
            Some(c) => c,
            None => return Ok(None),
        };

        let entries: Vec<DirectoryEntry> = children
            .iter()
            .map(|(name, node)| {
                let entry_path = if path.is_empty() {
                    name.clone()
                } else {
                    format!("{}/{}", path, name)
                };
                match node {
                    TreeNode::Directory(_) => DirectoryEntry::Dir(DirEntry {
                        name: name.clone(),
                        path: entry_path,
                    }),
                    TreeNode::File(f) => DirectoryEntry::File(FileEntry {
                        name: name.clone(),
                        path: entry_path,
                        size: f.size(),
                        executable: f.executable,
                        fingerprint: None,
                    }),
                }
            })
            .collect();

        Ok(Some(entries))
    }
}

// =============================================================================
// FileDest Implementation
// =============================================================================

#[async_trait]
impl crate::file_store::FileDest for MemoryFileStore {
    async fn get_entry(&self, path: &Path) -> Result<Option<DirectoryEntry>> {
        FileSource::get_entry(self, path).await
    }

    async fn list_directory(&self, path: &Path) -> Result<Option<DirectoryList>> {
        let path_str = MemoryFileStore::path_to_string(path);
        let root = self.root.read().await;

        // Check path exists and is a directory (or root)
        if !MemoryFileStore::is_root_path(path) {
            match get_node(&root, path) {
                Some(TreeNode::Directory(_)) => {}
                Some(TreeNode::File(_)) => {
                    return Err(Error::NotADirectory(path_str.clone()));
                }
                None => return Ok(None),
            }
        }

        let lister: Arc<dyn DirectoryListSource> =
            Arc::new(MemoryDirectoryListSource { root: root.clone() });

        drop(root);

        // Create and initialize ignore helper
        let mut helper = ScanIgnoreHelper::new();
        helper.initialize_to_path(Some(path), lister.as_ref()).await;

        // List raw entries
        let raw_entries = match lister.list_raw_directory(&path_str).await? {
            Some(entries) => entries,
            None => return Ok(None),
        };

        Ok(Some(DirectoryList::new(
            raw_entries,
            helper,
            lister,
            path_str,
        )))
    }

    async fn write_file_from_chunks(
        &self,
        path: &Path,
        mut chunks: SourceChunksWithContent,
        executable: bool,
    ) -> Result<()> {
        let components = path_components(path);
        if components.is_empty() {
            return Err(Error::InvalidPath("empty path".into()));
        }

        // Collect all chunk data before acquiring the write lock
        let mut data = Vec::new();
        while let Some(chunk_result) = chunks.next().await {
            let chunk = chunk_result?;
            let content = chunk.content().await?;
            data.extend_from_slice(&content.bytes[..]);
        }

        let mut root = self.root.write().await;
        let parent = get_or_create_parent(&mut root, &components[..components.len() - 1])?;
        let name = components.last().unwrap().to_string();
        parent.insert(
            name,
            TreeNode::File(MemoryFile {
                contents: MemoryFileContents::Explicit(Arc::new(data)),
                executable,
            }),
        );

        Ok(())
    }

    async fn rm(&self, path: &Path) -> Result<()> {
        let components = path_components(path);
        if components.is_empty() {
            // Remove everything from root
            let mut root = self.root.write().await;
            root.clear();
            return Ok(());
        }

        let mut root = self.root.write().await;

        if components.len() == 1 {
            // Remove from root level
            root.remove(components[0]);
            return Ok(());
        }

        // Navigate to parent and remove the child
        let parent_components = &components[..components.len() - 1];
        let mut current = &mut *root;
        for &component in parent_components {
            match current.get_mut(component) {
                Some(TreeNode::Directory(children)) => current = children,
                _ => return Ok(()), // Parent doesn't exist, nothing to remove
            }
        }
        current.remove(*components.last().unwrap());

        Ok(())
    }

    async fn mkdir(&self, path: &Path) -> Result<()> {
        let components = path_components(path);
        if components.is_empty() {
            return Ok(()); // Root already exists
        }

        let mut root = self.root.write().await;
        let mut current = &mut *root;
        for &component in &components {
            current = match current
                .entry(component.to_string())
                .or_insert_with(|| TreeNode::Directory(BTreeMap::new()))
            {
                TreeNode::Directory(children) => children,
                TreeNode::File(_) => {
                    return Err(Error::NotAFile(component.to_string()));
                }
            };
        }

        Ok(())
    }

    async fn set_executable(&self, path: &Path, executable: bool) -> Result<()> {
        let mut root = self.root.write().await;
        let node = get_node_mut(&mut root, path)
            .ok_or_else(|| Error::NotFound(Self::path_to_string(path)))?;

        match node {
            TreeNode::File(f) => {
                f.executable = executable;
                Ok(())
            }
            TreeNode::Directory(_) => Err(Error::NotAFile(Self::path_to_string(path))),
        }
    }
}

// =============================================================================
// Helper Functions
// =============================================================================

/// Recursively scan a tree with ignore filtering, generating scan events in depth-first, lexicographic order.
async fn scan_tree_with_ignore<S: super::scan_ignore_helper::ScanFileSource + ?Sized>(
    children: &BTreeMap<String, TreeNode>,
    current_path: PathBuf,
    events: &mut Vec<ScanEvent>,
    helper: &mut ScanIgnoreHelper,
    source: &S,
) {
    // BTreeMap iterates in sorted order
    for (name, node) in children {
        let is_dir = matches!(node, TreeNode::Directory(_));

        // Check if this entry should be ignored
        if helper.should_ignore(name, is_dir) {
            continue;
        }

        let entry_path = if current_path.as_os_str().is_empty() {
            PathBuf::from(name)
        } else {
            current_path.join(name)
        };
        let path_str = entry_path.to_string_lossy().into_owned();

        match node {
            TreeNode::Directory(dir_children) => {
                let dir_entry = DirEntry {
                    name: name.clone(),
                    path: path_str,
                };
                events.push(ScanEvent::EnterDirectory(dir_entry.clone()));

                // Notify helper of directory entry to load ignore files
                helper
                    .on_scan_event(
                        &ScanDirectoryEvent::EnterDirectory(ScanDirEntry::from(&dir_entry)),
                        source,
                    )
                    .await;

                Box::pin(scan_tree_with_ignore(
                    dir_children,
                    entry_path,
                    events,
                    helper,
                    source,
                ))
                .await;

                // Notify helper of directory exit
                helper
                    .on_scan_event(&ScanDirectoryEvent::ExitDirectory, source)
                    .await;

                events.push(ScanEvent::ExitDirectory);
            }
            TreeNode::File(file) => {
                events.push(ScanEvent::File(FileEntry {
                    name: name.clone(),
                    path: path_str,
                    size: file.size(),
                    executable: file.executable,
                    fingerprint: None,
                }));
            }
        }
    }
}

// =============================================================================
// Tests
// =============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    /// Helper to collect all events from a ScanEventList.
    async fn collect_scan_events(mut events: ScanEvents) -> Vec<ScanEvent> {
        let mut result = Vec::new();
        while let Some(event) = events.next().await {
            result.push(event.unwrap());
        }
        result
    }

    /// Helper to collect all chunks from a SourceChunkList.
    async fn collect_chunks(mut chunks: SourceChunks) -> Vec<SourceChunk> {
        let mut result = Vec::new();
        while let Some(chunk) = chunks.next().await {
            result.push(chunk.unwrap());
        }
        result
    }

    /// Helper to get entry using FileSource trait (avoids ambiguity with FileDest).
    async fn get_source_entry(store: &MemoryFileStore, path: &Path) -> Result<Option<DirectoryEntry>> {
        FileSource::get_entry(store, path).await
    }

    #[tokio::test]
    async fn test_empty_store() {
        let store = MemoryFileStore::builder().build();

        // Scan should yield no events
        let mut events = store.scan(None).await.unwrap();
        assert!(events.next().await.is_none());

        // Root should exist
        let root = get_source_entry(&store,Path::new("")).await.unwrap();
        assert!(matches!(root, Some(DirectoryEntry::Dir(_))));

        // Non-existent path
        let missing = get_source_entry(&store,Path::new("missing")).await.unwrap();
        assert!(missing.is_none());
    }

    #[tokio::test]
    async fn test_single_file() {
        let store = MemoryFileStore::builder()
            .add("hello.txt", MemoryFsEntry::file("Hello, World!"))
            .build();

        // Check entry
        let entry = get_source_entry(&store,Path::new("hello.txt")).await.unwrap();
        match entry {
            Some(DirectoryEntry::File(f)) => {
                assert_eq!(f.name, "hello.txt");
                assert_eq!(f.size, 13);
                assert!(!f.executable);
            }
            _ => panic!("Expected file entry"),
        }

        // Read file contents
        let contents = store.get_file(Path::new("hello.txt")).await.unwrap();
        assert_eq!(&contents[..], b"Hello, World!");

        // Scan
        let events = collect_scan_events(store.scan(None).await.unwrap()).await;
        assert_eq!(events.len(), 1);
        assert!(matches!(&events[0], ScanEvent::File(f) if f.name == "hello.txt"));
    }

    #[tokio::test]
    async fn test_nested_directories() {
        let store = MemoryFileStore::builder()
            .add("a/b/c.txt", MemoryFsEntry::file("nested"))
            .add("a/d.txt", MemoryFsEntry::file("sibling"))
            .build();

        // Check nested file
        let entry = get_source_entry(&store,Path::new("a/b/c.txt")).await.unwrap();
        assert!(matches!(entry, Some(DirectoryEntry::File(_))));

        // Check directory
        let dir = get_source_entry(&store,Path::new("a/b")).await.unwrap();
        assert!(matches!(dir, Some(DirectoryEntry::Dir(_))));

        // Scan should be depth-first, lexicographic
        let events = collect_scan_events(store.scan(None).await.unwrap()).await;

        // Expected order: EnterDir(a), EnterDir(b), File(c.txt), ExitDir, File(d.txt), ExitDir
        assert_eq!(events.len(), 6);
        assert!(matches!(&events[0], ScanEvent::EnterDirectory(d) if d.name == "a"));
        assert!(matches!(&events[1], ScanEvent::EnterDirectory(d) if d.name == "b"));
        assert!(matches!(&events[2], ScanEvent::File(f) if f.name == "c.txt"));
        assert!(matches!(&events[3], ScanEvent::ExitDirectory));
        assert!(matches!(&events[4], ScanEvent::File(f) if f.name == "d.txt"));
        assert!(matches!(&events[5], ScanEvent::ExitDirectory));
    }

    #[tokio::test]
    async fn test_executable_file() {
        let store = MemoryFileStore::builder()
            .add("script.sh", MemoryFsEntry::executable("#!/bin/bash"))
            .build();

        let entry = get_source_entry(&store,Path::new("script.sh")).await.unwrap();
        match entry {
            Some(DirectoryEntry::File(f)) => {
                assert!(f.executable);
            }
            _ => panic!("Expected file entry"),
        }
    }

    #[tokio::test]
    async fn test_repeated_file() {
        let store = MemoryFileStore::builder()
            .add("large.bin", MemoryFsEntry::repeated(b"ABCD", 1000))
            .build();

        let contents = store.get_file(Path::new("large.bin")).await.unwrap();
        assert_eq!(contents.len(), 1000);

        // Check pattern repeats correctly
        for (i, &byte) in contents.iter().enumerate() {
            let expected = b"ABCD"[i % 4];
            assert_eq!(byte, expected, "Mismatch at position {}", i);
        }
    }

    #[tokio::test]
    async fn test_chunking_small_file() {
        let store = MemoryFileStore::builder()
            .add("small.txt", MemoryFsEntry::file("tiny"))
            .build();

        // Get chunks with content directly from path
        let mut chunks_with_content = store
            .get_source_chunks_with_content(Path::new("small.txt"))
            .await
            .unwrap()
            .unwrap();

        let chunk = chunks_with_content.next().await.unwrap().unwrap();
        assert_eq!(chunk.offset, 0);
        assert_eq!(chunk.size, 4);

        let content = chunk.content().await.unwrap();
        assert_eq!(&content.bytes[..], b"tiny");

        // Should be only one chunk
        assert!(chunks_with_content.next().await.is_none());
    }

    #[tokio::test]
    async fn test_chunking_large_file() {
        // Create a file larger than 4MB to test chunking
        let size = 5_000_000u64; // ~5MB
        let store = MemoryFileStore::builder()
            .add("large.bin", MemoryFsEntry::repeated(b"X", size))
            .build();

        // First, check chunk metadata
        let chunks = collect_chunks(
            store
                .get_source_chunks(Path::new("large.bin"))
                .await
                .unwrap()
                .unwrap(),
        )
        .await;

        // First chunk should be 4MB
        assert_eq!(chunks[0].offset, 0);
        assert_eq!(chunks[0].size, 4_194_304);

        // Remaining ~806KB should be broken down further
        let total_size: u64 = chunks.iter().map(|c| c.size).sum();
        assert_eq!(total_size, size);

        // Verify chunk content hashes are computed
        let mut chunks_with_content = store
            .get_source_chunks_with_content(Path::new("large.bin"))
            .await
            .unwrap()
            .unwrap();
        let first_chunk = chunks_with_content.next().await.unwrap().unwrap();
        let content = first_chunk.content().await.unwrap();
        assert!(!content.hash.is_empty());
        assert_eq!(content.hash.len(), 64); // SHA-256 hex
    }

    #[tokio::test]
    async fn test_lexicographic_order() {
        let store = MemoryFileStore::builder()
            .add("z.txt", MemoryFsEntry::file("z"))
            .add("a.txt", MemoryFsEntry::file("a"))
            .add("m.txt", MemoryFsEntry::file("m"))
            .build();

        let events = collect_scan_events(store.scan(None).await.unwrap()).await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert_eq!(names, vec!["a.txt", "m.txt", "z.txt"]);
    }

    #[tokio::test]
    async fn test_get_source_chunks_not_found() {
        let store = MemoryFileStore::builder().build();
        let result = store.get_source_chunks(Path::new("missing")).await.unwrap();
        assert!(result.is_none());
    }

    #[tokio::test]
    async fn test_get_source_chunks_on_directory() {
        let store = MemoryFileStore::builder()
            .add("dir/file.txt", MemoryFsEntry::file("content"))
            .build();

        let result = store.get_source_chunks(Path::new("dir")).await;
        assert!(matches!(result, Err(Error::NotAFile(_))));
    }

    #[tokio::test]
    async fn test_get_file_not_found() {
        let store = MemoryFileStore::builder().build();
        let result = store.get_file(Path::new("missing")).await;
        assert!(matches!(result, Err(Error::NotFound(_))));
    }

    #[tokio::test]
    async fn test_get_file_on_directory() {
        let store = MemoryFileStore::builder()
            .add("dir/file.txt", MemoryFsEntry::file("content"))
            .build();

        let result = store.get_file(Path::new("dir")).await;
        assert!(matches!(result, Err(Error::NotAFile(_))));
    }

    // =========================================================================
    // Ignore Functionality Tests
    // =========================================================================

    #[tokio::test]
    async fn test_scan_ignores_git_directory() {
        let store = MemoryFileStore::builder()
            .add(".git/config", MemoryFsEntry::file("git config"))
            .add(".git/HEAD", MemoryFsEntry::file("ref: refs/heads/main"))
            .add("src/main.rs", MemoryFsEntry::file("fn main() {}"))
            .build();

        let events = collect_scan_events(store.scan(None).await.unwrap()).await;

        // .git directory should not appear in scan
        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::EnterDirectory(d) => Some(d.name.as_str()),
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert!(!names.contains(&".git"));
        assert!(names.contains(&"src"));
        assert!(names.contains(&"main.rs"));
    }

    #[tokio::test]
    async fn test_scan_ignores_tfs_directory() {
        let store = MemoryFileStore::builder()
            .add(".tfs/data", MemoryFsEntry::file("tfs data"))
            .add("file.txt", MemoryFsEntry::file("content"))
            .build();

        let events = collect_scan_events(store.scan(None).await.unwrap()).await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::EnterDirectory(d) => Some(d.name.as_str()),
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert!(!names.contains(&".tfs"));
        assert!(names.contains(&"file.txt"));
    }

    #[tokio::test]
    async fn test_scan_respects_gitignore() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log\ntarget/"))
            .add("app.log", MemoryFsEntry::file("log content"))
            .add("main.rs", MemoryFsEntry::file("fn main() {}"))
            .add("target/debug/app", MemoryFsEntry::file("binary"))
            .build();

        let events = collect_scan_events(store.scan(None).await.unwrap()).await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::EnterDirectory(d) => Some(d.name.as_str()),
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        // .gitignore itself should be visible
        assert!(names.contains(&".gitignore"));
        // Ignored files should not appear
        assert!(!names.contains(&"app.log"));
        assert!(!names.contains(&"target"));
        // Non-ignored files should appear
        assert!(names.contains(&"main.rs"));
    }

    #[tokio::test]
    async fn test_scan_respects_tfsignore() {
        let store = MemoryFileStore::builder()
            .add(".tfsignore", MemoryFsEntry::file("*.tmp"))
            .add("data.tmp", MemoryFsEntry::file("temp"))
            .add("data.txt", MemoryFsEntry::file("permanent"))
            .build();

        let events = collect_scan_events(store.scan(None).await.unwrap()).await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert!(!names.contains(&"data.tmp"));
        assert!(names.contains(&"data.txt"));
    }

    #[tokio::test]
    async fn test_scan_nested_gitignore() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log"))
            .add("src/.gitignore", MemoryFsEntry::file("*.bak"))
            .add("root.log", MemoryFsEntry::file("ignored"))
            .add("root.txt", MemoryFsEntry::file("visible"))
            .add("src/file.bak", MemoryFsEntry::file("ignored"))
            .add("src/file.rs", MemoryFsEntry::file("visible"))
            .add("src/app.log", MemoryFsEntry::file("also ignored"))
            .build();

        let events = collect_scan_events(store.scan(None).await.unwrap()).await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        // Root level ignore
        assert!(!names.contains(&"root.log"));
        assert!(names.contains(&"root.txt"));

        // Nested ignore (both root and nested patterns apply)
        assert!(!names.contains(&"file.bak"));
        assert!(!names.contains(&"app.log"));
        assert!(names.contains(&"file.rs"));
    }

    #[tokio::test]
    async fn test_scan_negation_pattern() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log\n!important.log"))
            .add("debug.log", MemoryFsEntry::file("ignored"))
            .add("important.log", MemoryFsEntry::file("not ignored"))
            .build();

        let events = collect_scan_events(store.scan(None).await.unwrap()).await;

        let names: Vec<_> = events
            .iter()
            .filter_map(|e| match e {
                ScanEvent::File(f) => Some(f.name.as_str()),
                _ => None,
            })
            .collect();

        assert!(!names.contains(&"debug.log"));
        assert!(names.contains(&"important.log"));
    }

    // =========================================================================
    // DirectoryList Tests
    // =========================================================================

    /// Helper to collect all entry names from a DirectoryList.
    async fn collect_names(list: &mut crate::file_store::DirectoryList) -> Vec<String> {
        let mut names = Vec::new();
        while let Some(Ok(entry)) = list.next().await {
            match entry {
                DirectoryEntry::Dir(d) => names.push(d.name),
                DirectoryEntry::File(f) => names.push(f.name),
            }
        }
        names
    }

    /// Helper to check if a name is in a list of names.
    fn has_name(names: &[String], name: &str) -> bool {
        names.iter().any(|n| n == name)
    }

    #[tokio::test]
    async fn test_directory_list_filters_ignored_entries() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log\ntarget/"))
            .add("app.log", MemoryFsEntry::file("log"))
            .add("main.rs", MemoryFsEntry::file("fn main() {}"))
            .add("target/debug/app", MemoryFsEntry::file("binary"))
            .build();

        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();
        let mut list = dest.list_directory(Path::new("")).await.unwrap().unwrap();

        let names = collect_names(&mut list).await;

        assert!(has_name(&names, ".gitignore"));
        assert!(has_name(&names, "main.rs"));
        assert!(!has_name(&names, "app.log"));
        assert!(!has_name(&names, "target"));
    }

    #[tokio::test]
    async fn test_directory_list_filters_git_and_tfs_dirs() {
        let store = MemoryFileStore::builder()
            .add(".git/config", MemoryFsEntry::file("config"))
            .add(".tfs/data", MemoryFsEntry::file("data"))
            .add("file.txt", MemoryFsEntry::file("content"))
            .build();

        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();
        let mut list = dest.list_directory(Path::new("")).await.unwrap().unwrap();

        let names = collect_names(&mut list).await;

        assert!(!has_name(&names, ".git"));
        assert!(!has_name(&names, ".tfs"));
        assert!(has_name(&names, "file.txt"));
    }

    #[tokio::test]
    async fn test_directory_list_child_carries_ignore_context() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log"))
            .add("src/.gitignore", MemoryFsEntry::file("*.bak"))
            .add("src/main.rs", MemoryFsEntry::file("fn main() {}"))
            .add("src/debug.log", MemoryFsEntry::file("log"))
            .add("src/backup.bak", MemoryFsEntry::file("backup"))
            .build();

        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();
        let root_list = dest.list_directory(Path::new("")).await.unwrap().unwrap();

        // Drill into src/ via list_directory
        let mut src_list = root_list.list_directory("src").await.unwrap().unwrap();
        let names = collect_names(&mut src_list).await;

        // Root ignore (*.log) and src ignore (*.bak) should both apply
        assert!(has_name(&names, ".gitignore"));
        assert!(has_name(&names, "main.rs"));
        assert!(!has_name(&names, "debug.log"));
        assert!(!has_name(&names, "backup.bak"));
    }

    #[tokio::test]
    async fn test_directory_list_nested_list_directory() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log"))
            .add("a/.gitignore", MemoryFsEntry::file("*.tmp"))
            .add("a/b/.tfsignore", MemoryFsEntry::file("*.bak"))
            .add("a/b/file.txt", MemoryFsEntry::file("content"))
            .add("a/b/file.log", MemoryFsEntry::file("ignored by root"))
            .add("a/b/file.tmp", MemoryFsEntry::file("ignored by a"))
            .add("a/b/file.bak", MemoryFsEntry::file("ignored by a/b"))
            .build();

        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();
        let root_list = dest.list_directory(Path::new("")).await.unwrap().unwrap();
        let a_list = root_list.list_directory("a").await.unwrap().unwrap();
        let mut b_list = a_list.list_directory("b").await.unwrap().unwrap();

        let names = collect_names(&mut b_list).await;

        // All three levels of ignore should apply
        assert!(has_name(&names, ".tfsignore"));
        assert!(has_name(&names, "file.txt"));
        assert!(!has_name(&names, "file.log"));
        assert!(!has_name(&names, "file.tmp"));
        assert!(!has_name(&names, "file.bak"));
    }

    #[tokio::test]
    async fn test_directory_list_nonexistent_child() {
        let store = MemoryFileStore::builder()
            .add("file.txt", MemoryFsEntry::file("content"))
            .build();

        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();
        let root_list = dest.list_directory(Path::new("")).await.unwrap().unwrap();

        let result = root_list.list_directory("nonexistent").await.unwrap();
        assert!(result.is_none());
    }

    #[tokio::test]
    async fn test_directory_list_from_subdirectory() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log"))
            .add("src/main.rs", MemoryFsEntry::file("fn main() {}"))
            .add("src/debug.log", MemoryFsEntry::file("log"))
            .build();

        // List starting from a subdirectory - root ignores should still apply
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();
        let mut list = dest
            .list_directory(Path::new("src"))
            .await
            .unwrap()
            .unwrap();

        let names = collect_names(&mut list).await;

        assert!(has_name(&names, "main.rs"));
        assert!(!has_name(&names, "debug.log"));
    }

    #[tokio::test]
    async fn test_directory_list_negation_pattern() {
        let store = MemoryFileStore::builder()
            .add(".gitignore", MemoryFsEntry::file("*.log\n!important.log"))
            .add("debug.log", MemoryFsEntry::file("ignored"))
            .add("important.log", MemoryFsEntry::file("kept"))
            .build();

        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();
        let mut list = dest.list_directory(Path::new("")).await.unwrap().unwrap();

        let names = collect_names(&mut list).await;

        assert!(!has_name(&names, "debug.log"));
        assert!(has_name(&names, "important.log"));
    }

    // =========================================================================
    // FileDest Mutation Tests
    // =========================================================================

    /// Helper to create a SourceChunksWithContent from bytes for testing write_file_from_chunks.
    fn chunks_with_content_from_bytes(data: &[u8]) -> SourceChunksWithContent {
        let managed_buffers = ManagedBuffers::new();
        let file = MemoryFile {
            contents: MemoryFileContents::Explicit(Arc::new(data.to_vec())),
            executable: false,
        };
        let file_size = file.size();
        Box::new(super::MemorySourceChunkWithContentList::new(
            file,
            file_size,
            managed_buffers,
        ))
    }

    #[tokio::test]
    async fn test_write_file_from_chunks() {
        let store = MemoryFileStore::builder().build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        let data = b"Hello, World!";
        let chunks = chunks_with_content_from_bytes(data);
        dest.write_file_from_chunks(Path::new("hello.txt"), chunks, false)
            .await
            .unwrap();

        // Verify via FileSource
        let contents = store.get_file(Path::new("hello.txt")).await.unwrap();
        assert_eq!(&contents[..], data);
    }

    #[tokio::test]
    async fn test_write_file_creates_parent_dirs() {
        let store = MemoryFileStore::builder().build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        let chunks = chunks_with_content_from_bytes(b"nested");
        dest.write_file_from_chunks(Path::new("a/b/c.txt"), chunks, false)
            .await
            .unwrap();

        let contents = store.get_file(Path::new("a/b/c.txt")).await.unwrap();
        assert_eq!(&contents[..], b"nested");

        // Parent directories should exist
        let dir = get_source_entry(&store,Path::new("a/b")).await.unwrap();
        assert!(matches!(dir, Some(DirectoryEntry::Dir(_))));
    }

    #[tokio::test]
    async fn test_write_file_executable() {
        let store = MemoryFileStore::builder().build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        let chunks = chunks_with_content_from_bytes(b"#!/bin/bash");
        dest.write_file_from_chunks(Path::new("script.sh"), chunks, true)
            .await
            .unwrap();

        let entry = get_source_entry(&store,Path::new("script.sh")).await.unwrap();
        match entry {
            Some(DirectoryEntry::File(f)) => assert!(f.executable),
            _ => panic!("Expected file entry"),
        }
    }

    #[tokio::test]
    async fn test_write_file_overwrites_existing() {
        let store = MemoryFileStore::builder()
            .add("file.txt", MemoryFsEntry::file("old content"))
            .build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        let chunks = chunks_with_content_from_bytes(b"new content");
        dest.write_file_from_chunks(Path::new("file.txt"), chunks, false)
            .await
            .unwrap();

        let contents = store.get_file(Path::new("file.txt")).await.unwrap();
        assert_eq!(&contents[..], b"new content");
    }

    #[tokio::test]
    async fn test_rm_file() {
        let store = MemoryFileStore::builder()
            .add("file.txt", MemoryFsEntry::file("content"))
            .build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        dest.rm(Path::new("file.txt")).await.unwrap();

        let entry = get_source_entry(&store,Path::new("file.txt")).await.unwrap();
        assert!(entry.is_none());
    }

    #[tokio::test]
    async fn test_rm_directory() {
        let store = MemoryFileStore::builder()
            .add("dir/a.txt", MemoryFsEntry::file("a"))
            .add("dir/b.txt", MemoryFsEntry::file("b"))
            .build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        dest.rm(Path::new("dir")).await.unwrap();

        let entry = get_source_entry(&store,Path::new("dir")).await.unwrap();
        assert!(entry.is_none());
    }

    #[tokio::test]
    async fn test_rm_nonexistent_is_ok() {
        let store = MemoryFileStore::builder().build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        // Should not error
        dest.rm(Path::new("nonexistent")).await.unwrap();
    }

    #[tokio::test]
    async fn test_mkdir() {
        let store = MemoryFileStore::builder().build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        dest.mkdir(Path::new("a/b/c")).await.unwrap();

        let entry = get_source_entry(&store,Path::new("a/b/c")).await.unwrap();
        assert!(matches!(entry, Some(DirectoryEntry::Dir(_))));
    }

    #[tokio::test]
    async fn test_mkdir_existing_dir_is_ok() {
        let store = MemoryFileStore::builder()
            .add("dir/file.txt", MemoryFsEntry::file("content"))
            .build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        // Should not error
        dest.mkdir(Path::new("dir")).await.unwrap();

        // Original contents should still be there
        let contents = store.get_file(Path::new("dir/file.txt")).await.unwrap();
        assert_eq!(&contents[..], b"content");
    }

    #[tokio::test]
    async fn test_mkdir_error_if_file_exists() {
        let store = MemoryFileStore::builder()
            .add("file.txt", MemoryFsEntry::file("content"))
            .build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        let result = dest.mkdir(Path::new("file.txt")).await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_set_executable() {
        let store = MemoryFileStore::builder()
            .add("script.sh", MemoryFsEntry::file("#!/bin/bash"))
            .build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        // Set executable
        dest.set_executable(Path::new("script.sh"), true)
            .await
            .unwrap();
        let entry = get_source_entry(&store,Path::new("script.sh")).await.unwrap();
        match entry {
            Some(DirectoryEntry::File(f)) => assert!(f.executable),
            _ => panic!("Expected file entry"),
        }

        // Clear executable
        dest.set_executable(Path::new("script.sh"), false)
            .await
            .unwrap();
        let entry = get_source_entry(&store,Path::new("script.sh")).await.unwrap();
        match entry {
            Some(DirectoryEntry::File(f)) => assert!(!f.executable),
            _ => panic!("Expected file entry"),
        }
    }

    #[tokio::test]
    async fn test_set_executable_not_found() {
        let store = MemoryFileStore::builder().build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        let result = dest.set_executable(Path::new("missing"), true).await;
        assert!(matches!(result, Err(Error::NotFound(_))));
    }

    #[tokio::test]
    async fn test_set_executable_on_directory() {
        let store = MemoryFileStore::builder()
            .add("dir/file.txt", MemoryFsEntry::file("content"))
            .build();
        let dest: &dyn crate::file_store::FileDest = store.get_dest().unwrap();

        let result = dest.set_executable(Path::new("dir"), true).await;
        assert!(matches!(result, Err(Error::NotAFile(_))));
    }
}
